# -*- coding: utf-8 -*-
"""tamil_to_english.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KQWucXi7qiEVX0s7aOxGyEk1RlYSqT3q
"""

from google.colab import drive
drive.mount("/content/drive")

import pathlib 
import random
import string
import re
import numpy as np

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import TextVectorization

dataset_path = "/content/drive/MyDrive/explorer_main/final_translation/dataset/tam_to_eng.txt"

data_pairs = []

with open(dataset_path, "r", encoding="utf-8") as file:
  content = file.readlines()
  for c in content:
   data = c.strip().split("-")
   tam = data[0]
   eng = data[1]
   eng = "[start] " + eng + " [end]"
   tup = (tam, eng)
   data_pairs.append(tup)

random.shuffle(data_pairs)

split_val = int(0.8*len(data_pairs))
remaining = data_pairs[split_val:]
train_pairs = data_pairs[:split_val]
dev_pairs = remaining[:int(0.5*len(remaining))]
test_pairs = remaining[int(0.5*len(remaining)):]

print("size of train set ", len(train_pairs))
print("size of dev set ", len(dev_pairs))
print("size of test set ", len(test_pairs))

# hyperparams
sequence_length = 20
vocab_size = 15000
embed_dim = 256
dense_units = 2048
num_heads = 8
batch_size = 64

eng_vec = TextVectorization(max_tokens=vocab_size, output_mode="int", output_sequence_length=sequence_length+1)
tam_vec = TextVectorization(max_tokens=vocab_size, output_mode="int", output_sequence_length=sequence_length)

tam_corpus = [i[0] for i in data_pairs]
eng_corpus = [i[1] for i in data_pairs]
tam_vec.adapt(tam_corpus)
eng_vec.adapt(eng_corpus)
print(eng_corpus)

def create_dataset(tam, eng):
  tam = tam_vec(tam)
  eng = eng_vec(eng)
  return ({"encoder_inputs":tam, "decoder_inputs":eng[:,:-1]}, eng[:,1:])
  # return ((tam, eng[:, :-1]), eng[:, 1:])

def make(pairs):
  tam, eng = zip(*pairs)
  tam = list(tam)
  eng = list(eng)
  dataset = tf.data.Dataset.from_tensor_slices((tam, eng))
  dataset = dataset.batch(batch_size).map(create_dataset).shuffle(2048).prefetch(16).cache()
  return dataset

train_data = make(train_pairs)
dev_data = make(dev_pairs)

for inputs, targets in train_data.take(1):
    print(targets)
    print(f'inputs["encoder_inputs"].shape: {inputs["encoder_inputs"].shape}')
    print(f'inputs["decoder_inputs"].shape: {inputs["decoder_inputs"].shape}')
    print(f"targets.shape: {targets.shape}")

class Encoder(layers.Layer):
  def __init__(self, num_heads, embed_dim, dense_units, **kwargs):
    super().__init__(**kwargs)
    self.num_heads = num_heads
    self.embed_dim = embed_dim
    self.dense_units = dense_units
    self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
    self.ffn = keras.Sequential([
        layers.Dense(dense_units, activation="relu"), layers.Dense(embed_dim)
    ])
    self.layernorm1 = layers.LayerNormalization()
    self.layernorm2 = layers.LayerNormalization()
    self.support_masking = True

  def call(self, inputs, mask=None):
    if mask is not None:
      padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype="int32")
    attn_out = self.mha(query=inputs, value=inputs, key=inputs)
    layernorm1_out = self.layernorm1(inputs + attn_out)

    dense_out = self.ffn(layernorm1_out)
    layernorm2_out = self.layernorm2(layernorm1_out + dense_out)

    return layernorm2_out


    # def call(self, inputs, mask=None):
    #     if mask is not None:
    #         padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype="int32")
    #     attention_output = self.mha(
    #         query=inputs, value=inputs, key=inputs, attention_mask=padding_mask
    #     )
    #     proj_input = self.layernorm1(inputs + attention_output)
    #     proj_output = self.ffn(proj_input)
    #     return self.layernorm2(proj_input + proj_output)
  
class PositionalEmbedding(layers.Layer):
  def __init__(self, seq_len, vocab_size, embed_dim, **kwargs):
    super().__init__(**kwargs)
    self.word_embed = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
    self.pos_embed = layers.Embedding(input_dim=seq_len, output_dim=embed_dim)
    self.seq_len = seq_len
    self.embed_dim = embed_dim
    self.vocab_size = vocab_size
  
  def call(self, inputs):
    length = tf.shape(inputs)[-1]
    positions = tf.range(start=0, limit=length, delta=1)
    embed_words = self.word_embed(inputs)
    embed_pos = self.pos_embed(positions)
    return embed_words + embed_pos

class Decoder(layers.Layer):
  def __init__(self, num_heads, embed_dim, dense_units, **kwargs):
    super().__init__()
    self.num_heads = num_heads
    self.embed_dim = embed_dim
    self.dense_units = dense_units
    self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
    self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
    self.ffn = keras.Sequential([
        layers.Dense(dense_units, activation="relu"), layers.Dense(embed_dim)
    ])
    self.layernorm1 = layers.LayerNormalization()
    self.layernorm2 = layers.LayerNormalization()
    self.layernorm3 = layers.LayerNormalization()
    self.support_masking = True

  def call(self, inputs, encoder_outputs, mask=None):
    causal_mask = self.get_causal_attention_mask(inputs)
    if mask is not None:
      padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype="int32")
      padding_mask = tf.mininum(padding_mask, causal_mask)

    attn1_out = self.mha1(query=inputs, key=inputs, value=inputs, attention_mask=causal_mask)
    layernorm1_out = self.layernorm1(inputs + attn1_out)

    attn2_out = self.mha2(query=inputs, key=encoder_outputs, value=encoder_outputs)
    layernorm2_out = self.layernorm2(layernorm1_out + attn2_out)

    dense_out = self.ffn(layernorm2_out)
    layernorm3_out = self.layernorm3(layernorm2_out + dense_out)

    return layernorm3_out
  
  def get_causal_attention_mask(self, inputs):
    input_shape = tf.shape(inputs)
    batch_size, seq_len = input_shape[0], input_shape[1]
    i = tf.range(seq_len)[:, tf.newaxis]
    j = tf.range(seq_len)
    mask = tf.cast( i>=j , dtype="int32")
    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],axis=0)
    return tf.tile(mask, mult)


encoder_inputs = layers.Input(shape=(None,), dtype="int64", name="encoder_inputs")
x = PositionalEmbedding(seq_len=sequence_length, vocab_size=vocab_size, embed_dim=embed_dim)(encoder_inputs)
encoder_outputs = Encoder(num_heads=num_heads, embed_dim=embed_dim, dense_units=dense_units)(x)
encoder_model = keras.Model(inputs=encoder_inputs, outputs=encoder_outputs)

decoder_inputs = layers.Input(shape=(None,), dtype="int64", name="decoder_inputs")
encoderout_inputs = keras.Input(shape=(None, embed_dim), name="decoder_state_inputs")
x = PositionalEmbedding(seq_len=sequence_length, vocab_size=vocab_size, embed_dim=embed_dim)(decoder_inputs)
x = Decoder(num_heads=num_heads, embed_dim=embed_dim, dense_units=dense_units)(x, encoderout_inputs)
x = keras.layers.Dropout(rate=0.5)(x)
decoder_outputs = keras.layers.Dense(vocab_size, activation="softmax")(x)
decoder_model =  keras.Model(inputs=[decoder_inputs, encoderout_inputs], outputs=decoder_outputs)

decoder_outputs = decoder_model([decoder_inputs, encoder_outputs])

transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)

transformer.summary()
transformer.compile(loss="sparse_categorical_crossentropy", optimizer="rmsprop", metrics=["accuracy"])

wt_path = "/content/drive/MyDrive/explorer_main/translation_model_h5_format.h5"
transformer.load_weights(wt_path)

transformer.fit(train_data, epochs=20, validation_data=dev_data)

ta = data_pairs[0][0]
en = data_pairs[0][1]

encoder_in = tam_vec([ta])
decoder_in = eng_vec([en])[:, :-1]

# print(input)
# encoder_in = eng_vec(["[start] Students have a holiday on Foundation Day. [end]"])

eng_vocab = eng_vec.get_vocabulary()
eng_lookup = dict(zip(range(len(eng_vocab)), eng_vocab))
max_sen_len =  20

def decode_sentence(input):
  encoder_in = tam_vec([input])
  decoded_sentence = "[start] "
  for i in range(max_sen_len):
    decoder_in = eng_vec([decoded_sentence])[:, :-1]
    predictions = transformer([encoder_in, decoder_in])
    prediction_idx = np.argmax(predictions[0,i,:])
    word = eng_lookup[prediction_idx]
    decoded_sentence += " " + word
    if word=="[end]":
      break
  return decoded_sentence

test_tam = [i[0] for i in test_pairs]
test = "நீங்கள் வெளிர் நிறமாகத் தெரிகிறீர்கள்"
# for i in range(5):
#   input_sentence = random.choice(test_tam)
#   output = decode_sentence(input_sentence)
#   print(output)
output = decode_sentence(test)
print(output)

save_path = "/content/drive/MyDrive/explorer_main/final_translation/final_model"

transformer.save(f"{save_path}/h5_format/explorer.h5")
transformer.save(f"{save_path}/explorer_save_model_format")
transformer.save(f"{save_path}/explorer_saved_model_format")

transformer.save_weights(f"{save_path}/weights/explorer_wts.h5")

transformer.load_weights(f"{save_path}/weights/explorer_wts.h5")































